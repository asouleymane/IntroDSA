{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Practice with `Python`\n",
    "\n",
    "There is still a lot to learn about machine learning, and it is important to recognize that we have barely started to scrape the surface of it. There are many things we could do to refine our model that we didn't touch on in this module (don't worry, these will be covered throughout your curriculum), such as data transformation, elegant methods for automated feature selection, as well as unsupervised learning.\n",
    "\n",
    "For these exercises, we ask you to only complete **ONE** of the exercise notebooks, either `Python` or `R`. We will be asking you to predict wine quality using both Decision Tree and Naïve Bayes. Your exercises will serve as a sort-of extended practice in which you are free to try and refine the model however you see fit, but we do ask you to use both Decision Tree and Naïve Bayes.\n",
    "\n",
    "The questions will guide you a bit, but if you want to experiment or you find, through data exploration, a model that is better, feel free to do so. If you go this route, leave comments in the code justifying why you did what you did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data\n",
    "\n",
    "Today we will be using the Red Wine Quality data. The target variable is numeric, so we are going to discretize it a bit before we get to the activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../../datasets/wine-quality/winequality-red.csv') as file:\n",
    "    df = pd.read_csv(file, delimiter=\";\")\n",
    "    # if wine quality is less than 6, assign the value \"bad\".\n",
    "    # if greater than 6, assign \"good\". \n",
    "    # 6 is the most popular value by a lot in this set, so \n",
    "    # we are going to assign it a unique value. We will call \n",
    "    # this \"normal\" as it is in the middle of the distribution.\n",
    "    \n",
    "    vals_to_replace = {3:'bad', 4:'bad', 5:'bad', 6:'okay', 7:'good', 8:'good',9:'good'}\n",
    "    df['quality'] = df['quality'].map(vals_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Create a training data set and testing data set from the `wine` data frame. Make sure that the rows are randomly selected. The training set should be constructed from 60% of the data; call it `train`. The testing set should be called `test` and should be constructed from the **other** 40% of the data. Be sure to set the `random_state` to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for exercise 1 goes here\n",
    "# *****************************\n",
    "train = df.sample(n = 959,random_state = 1)\n",
    "test = df.drop(train.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Create `numpy` arrays for both the input variables and the target variables. The target should be the `quality` variable. Use all of the values for the input, other than the target variable. Do this for both the training and testing set. Call the inputs for the training set `train_X` and the target `train_y`, and the inputs for the testing set `test_X` and `test_y` for the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed acidity',\n",
       " 'volatile acidity',\n",
       " 'citric acid',\n",
       " 'residual sugar',\n",
       " 'chlorides',\n",
       " 'free sulfur dioxide',\n",
       " 'total sulfur dioxide',\n",
       " 'density',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'quality']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "col_names=list(df)\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for exercise 2 goes here\n",
    "# *****************************\n",
    "train_X = np.asarray(train[['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide',\n",
    " 'total sulfur dioxide','density','pH','sulphates','alcohol']])\n",
    "train_y = np.asarray(train.quality)\n",
    "test_X = np.asarray(test[['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide',\n",
    " 'total sulfur dioxide','density','pH','sulphates','alcohol']])\n",
    "test_y = np.asarray(test.quality)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Exercise 3**: Create a Decision Tree model from the `tree` module. Make sure you name the classifier something (in the other notebooks, we called it `clf`). Then train the classifier using the `fit()` method, and pass the `train_X` and `train_y` as the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for exercise 3 goes here\n",
    "# *****************************\n",
    "clasfr = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clasfr = clasfr.fit(train_X, train_y )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: What is the misclassification error rate of the tree using the **testing** set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 640 points : 243\n"
     ]
    }
   ],
   "source": [
    "# Code for exercise 4 goes here\n",
    "# *****************************\n",
    "y_pred = clasfr.fit(train_X, train_y).predict(test_X)\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\"\n",
    "      .format(len(test),(test_y != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Find the feature importances by running the method `feature_importances_` on the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fixed acidity', 0.051041837210308992),\n",
       " ('volatile acidity', 0.12579481929857203),\n",
       " ('citric acid', 0.083838053375543536),\n",
       " ('residual sugar', 0.077147675505427338),\n",
       " ('chlorides', 0.073182853068002374),\n",
       " ('free sulfur dioxide', 0.079682912504679029),\n",
       " ('total sulfur dioxide', 0.067580892633803027),\n",
       " ('density', 0.038021202097473811),\n",
       " ('pH', 0.067632343146992471),\n",
       " ('sulphates', 0.13591682740414107),\n",
       " ('alcohol', 0.20016058375505633)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for exercise 5 goes here\n",
    "# *****************************\n",
    "z = zip(col_names[0:11],clasfr.feature_importances_)\n",
    "list(z)\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**: Create a Naïve Bayes model. Make sure you name the classifier something (in the other notebooks, we called it `nbc`). Then train the classifier using the `fit()` method, and pass the `train_X` and `train_y` as the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for exercise 6 goes here\n",
    "# *****************************\n",
    "nbm = GaussianNB()\n",
    "nbm.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: What is the misclassification error rate of the Naïve Bayes classifier using the **testing** set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 640 points : 244\n"
     ]
    }
   ],
   "source": [
    "# Code for exercise 7 goes here\n",
    "# *****************************\n",
    "y_pred = nbm.fit(train_X,train_y).predict(test_X)\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\"\n",
    "      .format(test.shape[0],(test_y != y_pred).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**: Create a subset of the original data frame `df` to include the top 5 features (from the feature importances listed above) and the target variable `quality`. Call this new data frame something other than `df`.\n",
    "\n",
    "Then create training and testing sets on this new data frame using the same method as in Exercise 1. Then create your new training and testing inputs and targets using the method in Exercise 2. Be sure to name these objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for exercise 8 goes here\n",
    "# *****************************\n",
    "df2 =df.ix[:,[1,2,5,9,10,11]]\n",
    "df2.head()\n",
    "train_X1 = np.asarray(train[['volatile acidity','citric acid','free sulfur dioxide','sulphates','alcohol']])\n",
    "train_y1 = np.asarray(train.quality)\n",
    "test_X1 = np.asarray(test[['volatile acidity','citric acid','free sulfur dioxide','sulphates','alcohol']])\n",
    "test_y1 = np.asarray(test.quality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 9**: Now create a new Naïve Bayes classifier and train it on using your new training data created in exercise 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for exercise 9 goes here\n",
    "# *****************************\n",
    "nbm1 = GaussianNB()\n",
    "nbm1.fit(train_X1,train_y1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**: Does using only these select features create a better model according to the testing data misclassification error rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 640 points : 234\n"
     ]
    }
   ],
   "source": [
    "# Code for exercise 10 goes here\n",
    "# *****************************\n",
    "y_pred = nbm1.fit(train_X1,train_y1).predict(test_X1)\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\"\n",
    "      .format(test.shape[0],(test_y1 != y_pred).sum()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "here we have 234 mislabeled compared to 244 for the model including all the columns so definitely the 5 columns with the highest feature importances will eventually produce less mislabeled points.234 compared to 244."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
